{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroGuardX: An Explainable AI Pipeline for Intrusion Detection\\n",
    "\\n",
    "This notebook provides a tutorial for the NeuroGuardX system, an Explainable AI (XAI) pipeline for intrusion detection in Online Social Networks (OSN) and Network Systems. We will walk through each step of the pipeline, from data preprocessing to model explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\\n",
    "import numpy as np\\n",
    "from scipy import stats\\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\\n",
    "import requests\\n",
    "import os\\n",
    "from sklearn.decomposition import PCA\\n",
    "from sklearn.manifold import TSNE\\n",
    "from sklearn.feature_selection import RFE\\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n",
    "from sklearn.svm import SVC\\n",
    "from sklearn.model_selection import train_test_split\\n",
    "from tensorflow.keras.models import Sequential, Model\\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\\n",
    "from tensorflow.keras.optimizers import Adam\\n",
    "import shap\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "\\n",
    "print('Setup Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Download\\n",
    "\\n",
    "This notebook uses the NSL-KDD dataset. We will download it from a public repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\\n",
    "    os.makedirs('data')\\n",
    "\\n",
    "url = 'https://figshare.com/ndownloader/files/54839969'\\n",
    "response = requests.get(url)\\n",
    "with open('data/Train_data.csv', 'wb') as f:\\n",
    "    f.write(response.content)\\n",
    "\\n",
    "# Create a dummy Test_data.csv for demonstration purposes, as a separate test file is not available from the source.\\n",
    "train_df = pd.read_csv('data/Train_data.csv')\\n",
    "test_df = train_df.sample(frac=0.2, random_state=42)\\n",
    "test_df.to_csv('data/Test_data.csv', index=False)\\n",
    "\\n",
    "print('Data downloaded and dummy test set created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\\n",
    "\\n",
    "In this section, we'll preprocess the OSN and Network datasets. The preprocessing pipeline consists of three main steps:\\n",
    "\\n",
    "1.  **Data Collection:** We load the data from the `data` directory.\\n",
    "2.  **Data Cleaning:** We handle missing values and remove outliers.\\n",
    "3.  **Data Transformation:** We scale numerical features and one-hot encode categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(dataset_path):\\n",
    "    try:\\n",
    "        df = pd.read_csv(dataset_path)\\n",
    "        print(f\"Successfully loaded data from local file: {dataset_path}\")\\n",
    "        return df\\n",
    "    except Exception as e:\\n",
    "        print(f\"An error occurred while collecting data from {dataset_path}: {e}\")\\n",
    "        return None\\n",
    "\\n",
    "def clean_data(df):\\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\\n",
    "    return df\\n",
    "\\n",
    "def transform_data(data, method):\\n",
    "    numeric_data = data.select_dtypes(include=np.number)\\n",
    "    categorical_data = data.select_dtypes(exclude=np.number)\\n",
    "\\n",
    "    if not numeric_data.empty:\\n",
    "        if method == \\\"Z-score\\\":\\n",
    "            scaler = StandardScaler()\\n",
    "        elif method == \\\"Min-Max\\\":\\n",
    "            scaler = MinMaxScaler()\\n",
    "        scaled_numeric_data = scaler.fit_transform(numeric_data)\\n",
    "        scaled_df = pd.DataFrame(scaled_numeric_data, columns=numeric_data.columns, index=numeric_data.index)\\n",
    "    else:\\n",
    "        scaled_df = pd.DataFrame()\\n",
    "\\n",
    "    if not categorical_data.empty:\\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\\n",
    "        encoded_categorical_data = encoder.fit_transform(categorical_data)\\n",
    "        encoded_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_data.columns), index=categorical_data.index)\\n",
    "    else:\\n",
    "        encoded_df = pd.DataFrame()\\n",
    "\\n",
    "    return pd.concat([scaled_df, encoded_df], axis=1)\\n",
    "\\n",
    "def preprocess_data(osn_path, network_path):\\n",
    "    # For the purpose of this notebook, we'll assign the NSL-KDD dataset to both OSN and Network roles.\\n",
    "    OSN_data = collect_data(osn_path)\\n",
    "    Network_data = collect_data(network_path)\\n",
    "\\n",
    "    if OSN_data is None or Network_data is None:\\n",
    "        return None, None, None, None\\n",
    "\\n",
    "    # Separate features and labels\\n",
    "    OSN_features = OSN_data.iloc[:, :-1]\\n",
    "    OSN_labels = OSN_data.iloc[:, -1]\\n",
    "    Network_features = Network_data.iloc[:, :-1]\\n",
    "    Network_labels = Network_data.iloc[:, -1]\\n",
    "\\n",
    "    # Encode labels\\n",
    "    le = LabelEncoder()\\n",
    "    OSN_labels_encoded = le.fit_transform(OSN_labels)\\n",
    "    Network_labels_encoded = le.fit_transform(Network_labels)\\n",
    "\\n",
    "    # Clean and transform features only\\n",
    "    OSN_cleaned_features = clean_data(OSN_features)\\n",
    "    Network_cleaned_features = clean_data(Network_features)\\n",
    "    OSN_transformed_features = transform_data(OSN_cleaned_features, \\\"Z-score\\\")\\n",
    "    Network_transformed_features = transform_data(Network_cleaned_features, \\\"Min-Max\\\")\\n",
    "\\n",
    "    return OSN_transformed_features, pd.Series(OSN_labels_encoded), Network_transformed_features, pd.Series(Network_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSN_dataset_path = 'data/Train_data.csv'\\n",
    "Network_dataset_path = 'data/Test_data.csv'\\n",
    "\\n",
    "OSN_processed_features, OSN_processed_labels, Network_processed_features, Network_processed_labels = preprocess_data(OSN_dataset_path, Network_dataset_path)\\n",
    "\\n",
    "print(\\\"OSN Processed Features Head:\\\")\\n",
    "print(OSN_processed_features.head())\\n",
    "\\n",
    "print(\\\"Network Processed Features Head:\\\")\\n",
    "print(Network_processed_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\\n",
    "sns.histplot(OSN_processed_features.iloc[:, 0], kde=True)\\n",
    "plt.title('Distribution of the First Feature in the OSN Dataset')\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\\n",
    "\\n",
    "In this section, we'll select the most important features from the preprocessed datasets. The feature selection pipeline consists of two main steps:\\n",
    "\\n",
    "1.  **Feature Importance:** We use Recursive Feature Elimination (RFE) to select the most important features.\\n",
    "2.  **Dimensionality Reduction:** We reduce the dimensionality of the selected features using PCA or t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(features, dataset_type):\\n",
    "    numeric_features = features.select_dtypes(include=np.number)\\n",
    "    non_numeric_features = features.select_dtypes(exclude=np.number)\\n",
    "\\n",
    "    if numeric_features.empty:\\n",
    "        return features\\n",
    "\\n",
    "    if dataset_type == \\\"OSN\\\" and numeric_features.shape[1] > 50:\\n",
    "        reducer = TSNE(n_components=2, perplexity=min(30, len(numeric_features)-1), random_state=42)\\n",
    "    else:\\n",
    "        reducer = PCA(n_components=2, random_state=42)\\n",
    "\\n",
    "    reduced_numeric_data = reducer.fit_transform(numeric_features)\\n",
    "    reduced_df = pd.DataFrame(reduced_numeric_data, index=numeric_features.index)\\n",
    "\\n",
    "    return pd.concat([reduced_df, non_numeric_features], axis=1)\\n",
    "\\n",
    "def select_important_features(features, labels):\\n",
    "    numeric_features = features.select_dtypes(include=np.number)\\n",
    "    non_numeric_features = features.select_dtypes(exclude=np.number)\\n",
    "\\n",
    "    if numeric_features.empty:\\n",
    "        return features\\n",
    "\\n",
    "    estimator = RandomForestClassifier(random_state=42)\\n",
    "    n_features_to_select = min(5, numeric_features.shape[1])\\n",
    "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\\n",
    "    selector = selector.fit(numeric_features, labels)\\n",
    "\\n",
    "    selected_numeric_features = numeric_features.loc[:, selector.support_]\\n",
    "\\n",
    "    return pd.concat([selected_numeric_features, non_numeric_features], axis=1)\\n",
    "\\n",
    "def feature_selection(features, labels, dataset_type):\\n",
    "    if features is None or features.empty:\\n",
    "        print(f\\\"Features for {dataset_type} is empty. Skipping feature selection.\\\")\\n",
    "        return None\\n",
    "\\n",
    "    X_important = select_important_features(features, labels)\\n",
    "    X_reduced = reduce_dimensionality(X_important, dataset_type)\\n",
    "\\n",
    "    print(f\\\"Feature selection complete for {dataset_type} dataset.\\\")\\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSN_features_selected = feature_selection(OSN_processed_features, OSN_processed_labels, \\\"OSN\\\")\\n",
    "Network_features_selected = feature_selection(Network_processed_features, Network_processed_labels, \\\"Network\\\")\\n",
    "\\n",
    "print(\\\"OSN Features Head:\\\")\\n",
    "print(OSN_features_selected.head())\\n",
    "\\n",
    "print(\\\"Network Features Head:\\\")\\n",
    "print(Network_features_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_learning_model(features, labels, dataset_type):\\n",
    "    if features is None or labels is None or features.empty:\\n",
    "        print(f\\\"Skipping deep learning model training for {dataset_type} due to missing data.\\\")\\n",
    "        return None\\n",
    "\\n",
    "    num_classes = len(np.unique(labels))\\n",
    "    model = Sequential([\\n",
    "        Input(shape=(features.shape[1],)),\\n",
    "        Dense(64, activation='relu'),\\n",
    "        Dense(32, activation='relu'),\\n",
    "        Dense(num_classes, activation='softmax')\\n",
    "    ])\\n",
    "\\n",
    "    optimizer = Adam(learning_rate=0.001)\\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n",
    "\\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features.values, labels.values, test_size=0.2, random_state=42)\\n",
    "\\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), verbose=1)\\n",
    "    return model\\n",
    "\\n",
    "osn_dl_model = train_deep_learning_model(OSN_features_selected, OSN_processed_labels, \\\"OSN\\\")\\n",
    "network_dl_model = train_deep_learning_model(Network_features_selected, Network_processed_labels, \\\"Network\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_rule(layer, R, a):\\n",
    "    w = layer.get_weights()[0]\\n",
    "    b = layer.get_weights()[1] if len(layer.get_weights()) > 1 else 0\\n",
    "    epsilon = 1e-7\\n",
    "    z = np.dot(a, w) + b\\n",
    "    s = R / (z + epsilon)\\n",
    "    c = np.dot(s, w.T)\\n",
    "    return a * c\\n",
    "\\n",
    "def lrp_explain(model, data):\\n",
    "    layer_outputs = [layer.output for layer in model.layers]\\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\\n",
    "    activations = activation_model.predict(data)\\n",
    "    R = model.predict(data)\\n",
    "    for i in range(len(model.layers) - 1, 0, -1):\\n",
    "        R = lrp_rule(model.layers[i], R, activations[i-1])\\n",
    "    return R\\n",
    "\\n",
    "def shap_explain(model, data, model_type='deep_learning'):\\n",
    "    if model_type == 'deep_learning':\\n",
    "        explainer = shap.GradientExplainer(model, data)\\n",
    "    elif model_type == 'tree':\\n",
    "        explainer = shap.TreeExplainer(model, data)\\n",
    "    else:\\n",
    "        explainer = shap.KernelExplainer(model.predict, data)\\n",
    "\\n",
    "    shap_values = explainer.shap_values(data)\\n",
    "\\n",
    "    if isinstance(shap_values, list):\\n",
    "        shap_values = shap_values[0]\\n",
    "\\n",
    "    return shap_values\\n",
    "\\n",
    "osn_lrp = lrp_explain(osn_dl_model, OSN_features_selected.values)\\n",
    "osn_shap = shap_explain(osn_dl_model, OSN_features_selected.values, 'deep_learning')\\n",
    "\\n",
    "print(\\\"OSN LRP Values Head:\\\")\\n",
    "print(pd.DataFrame(osn_lrp, columns=OSN_features_selected.columns).head())\\n",
    "\\n",
    "shap.summary_plot(osn_shap, OSN_features_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}